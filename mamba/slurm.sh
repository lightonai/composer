#!/bin/bash -l

#SBATCH --job-name=mamba                     # name of job
#SBATCH --partition=gpu                      # partition
#SBATCH --qos=default                        # quality of service
#SBATCH --nodes=1                            # total number of nodes
#SBATCH --ntasks=4                           # total number of tasks
#SBATCH --ntasks-per-node=4                  # number of tasks per single node
#SBATCH --account=account                    # account

#SBATCH --gpus-per-node=4                    # number of GPU per node
#SBATCH --gpus-per-task=1                    # number of GPU per node
#SBATCH --exclusive                          # job has exclusive use of the resource, no sharing
#SBATCH --wait-all-nodes=1
#SBATCH --time=00:20:00                      # maximum execution time requested (HH:MM:SS)
#SBATCH --output=mamba_stdout_single%j.out   # name of output file
#SBATCH --error=mamba_stderr_single%j.out    # name of error file

# W&B API key
export WANDB_API_KEY="your-api-key"

# clean & load modules
module purge
module load env/release/2023.1
module load CUDA/12.2.0
source ~/.bashrc

# activate venv
conda activate your-env

# EDIT: useful for debug if needed
#
# to debug NCCL issues
# export NCCL_DEBUG=INFO
#
# to unravel async errors w/o the correct traceback - potentially makes everything very slower
# export CUDA_LAUNCH_BLOCKING=1
#
# to force crashing on nccl issues like hanging broadcast
# export NCCL_ASYNC_ERROR_HANDLING=1

# config
# export NCCL_SOCKET_IFNAME=ib0
# export OMP_NUM_THREADS=8

NODES=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
NNODES=${#NODES[@]}
NODES_ARRAY=($NODES)
HEAD_NODE=${NODES_ARRAY[0]}
MASTER_ADDR=$(srun --nodes=1 --ntasks=1 -w "$HEAD_NODE" hostname --ip-address)
MASTER_PORT=$RANDOM
NPROC=$(nvidia-smi -L | wc -l)
WORLD_SIZE=$((NNODES * NPROC))

# Uncomment if you want to use the config generated by create_mamba_config.py
# srun --nodes=1 --ntasks=1 -w "$HEAD_NODE" python mamba/create_mamba_config.py

echo "Total number of nodes: $NNODES"
echo "Node names: ${NODES[@]}"
echo "Head node: $HEAD_NODE"

function run_composer() {
    srun --nodelist=${NODE} --ntasks=1 composer \
        --world_size ${WORLD_SIZE} \
        --nproc ${NPROC} \
        --node_rank ${NODE_RANK} \
        --master_addr ${MASTER_ADDR} \
        --master_port ${MASTER_PORT} \
        --verbose \
        mamba/train.py
}

NODE_RANK=1
for (( NODE_RANK=1; NODE_RANK<${NNODES}; NODE_RANK++ ))
do
    NODE=${NODES[$NODE_RANK]}
    echo "Run compute node ${NODE} for rank: ${NODE_RANK}"
    run_composer &
done

NODE_RANK=0
NODE=${HEAD_NODE}
echo "Run master node ${NODE} for rank: ${NODE_RANK}"
run_composer
wait