model:
  vocab_size: 64000
  d_model: 1536
  d_intermediate: 0
  n_layer: 48
  fsdp_layer_wrap: true
  activation_checkpointing: true
data:
  global_batch_size: 896
  micro_batch_size: 8
  eval_batch_size: 8
  seq_len: 512
  num_workers: 2
  prefetch_factor: 2
  n_total_tokens: 28276491625
  n_samples_to_skip: 0
  token_size: 2
  text_paths:
  - path: /home/data/mambarabic/tokenized_data/000_Arabic-data-merged.ds

optimizer:
  lr: 0.00045
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
scheduler:
  name: wsd
  t_warmup: 2827649162tok
  t_max: 28276491625tok
  t_decay: 2827649162tok
  alpha_f: 0.1
fsdp:
  sharding_strategy: FULL_SHARD
  process_group: null
  backward_prefetch: BACKWARD_PRE
  activation_checkpointing_reentrant: true
  activation_checkpointing: true
  limit_all_gathers: false
  verbose: true
trainer:
  cp_interval: 2298240tok
  eval_interval: 2298240tok
  save_interval: 2298240tok
  eval_subset_num_batches: 16
  save_folder: checkpoints
  auto_log_hparams: true
  autoresume: true
general:
  project_name: mamba-arabic
  full_name: dry-run-mambarabic-896-8
  window_size: 10
  clipping_threshold: 1.0
  position_weighting: true